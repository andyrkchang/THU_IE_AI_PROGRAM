{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_pytorch_CNN_v2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPE7eHRbeTzrDNwVH3EK8AH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a47a8570573d4c529cf66b01d1a7d346":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3e5520eaec744a6292f79f1cee9287ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c2b1c4ab4bd14438a06430b411fd3bb0","IPY_MODEL_bf59a40484fd4a83960fb3bb66b6920c"]}},"3e5520eaec744a6292f79f1cee9287ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c2b1c4ab4bd14438a06430b411fd3bb0":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0b6f74eaf25b4cfcb9433feee544e2ca","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3cb5253bef47460dbc3a6d7f8d9a0015"}},"bf59a40484fd4a83960fb3bb66b6920c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_55106fdaf17540e1bbc81a8332e78b3f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:20&lt;00:00, 2796555.80it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bdb5502719ea499d902043d3912681ae"}},"0b6f74eaf25b4cfcb9433feee544e2ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3cb5253bef47460dbc3a6d7f8d9a0015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"55106fdaf17540e1bbc81a8332e78b3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bdb5502719ea499d902043d3912681ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c1c38ce34794fbc8b427411a97efe35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ff9224f6e3a14956aa1b7034ab112fc7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_19ff6fef8216471ea2af3a713e040560","IPY_MODEL_e913c42f3a8249b38c31adc5ce3409c9"]}},"ff9224f6e3a14956aa1b7034ab112fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19ff6fef8216471ea2af3a713e040560":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5fbe35ac463249878373915da61b5410","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7cc299f0dd384f959e307c14f739f923"}},"e913c42f3a8249b38c31adc5ce3409c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b1ee4abffeae4baa92855629a0fe7302","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 32768/? [00:00&lt;00:00, 200045.05it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2a15eaecb1a450cb2be787e0e272a95"}},"5fbe35ac463249878373915da61b5410":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7cc299f0dd384f959e307c14f739f923":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b1ee4abffeae4baa92855629a0fe7302":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d2a15eaecb1a450cb2be787e0e272a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f5fc981f2b14357a830e9fab0a1d998":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d9d031a23e9c467fb8dbf8d4444ba0a5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f6d3b197446f4c26bcfb4e52f2ef9c0e","IPY_MODEL_3a089e5e85da49e8ad4b6ad1535d9b79"]}},"d9d031a23e9c467fb8dbf8d4444ba0a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f6d3b197446f4c26bcfb4e52f2ef9c0e":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c1d3ccb922d14f5184ce10b2705b3853","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8304600538e248418ebfbe9d8f92ce3e"}},"3a089e5e85da49e8ad4b6ad1535d9b79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52ea3f73d3434eba88ec06e720ef8390","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:19&lt;00:00, 1808030.78it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6869f33a735341c8897e395a02b45373"}},"c1d3ccb922d14f5184ce10b2705b3853":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8304600538e248418ebfbe9d8f92ce3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52ea3f73d3434eba88ec06e720ef8390":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6869f33a735341c8897e395a02b45373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"adca0ca99df74a68a6e87e783945d5b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a48a5644afa14bdd96d5db8e77fdfd15","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1a7e3014c2aa424993ae88ad1492f059","IPY_MODEL_0a590c433b584cf48020d22db376c071"]}},"a48a5644afa14bdd96d5db8e77fdfd15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a7e3014c2aa424993ae88ad1492f059":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1f4420659dfe4bc882ffcd365df1f22a","_dom_classes":[],"description":"  0%","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_26c29dbdfc6c4b3c81abc9040b6820a9"}},"0a590c433b584cf48020d22db376c071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_12ede7fcc62c43f090271fb071176560","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/4542 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_43f2f0b667304bb8b4bd84714a2c752d"}},"1f4420659dfe4bc882ffcd365df1f22a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"26c29dbdfc6c4b3c81abc9040b6820a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"12ede7fcc62c43f090271fb071176560":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"43f2f0b667304bb8b4bd84714a2c752d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"yGkRLd70CbhW","colab_type":"text"},"source":["# MNIST Pytorch Classification Sample Code\n","\n","2020/05/01, Andy RK Chang v1.0 initial \n","\n","2020/05/02, Andy RK Chang v2.0 revise parameter\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4gWT3rS_ES6y","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"9XLnqDzYYPeb","colab_type":"text"},"source":["MNIST interactive examples in pytorch and tensorflow\n","\n","https://github.com/pyliaorachel/MNIST-pytorch-tensorflow-eager-interactive\n"]},{"cell_type":"code","metadata":{"id":"oc7zfbC9Oxse","colab_type":"code","outputId":"522bb595-dd72-4e5b-b301-7aeedff3c335","executionInfo":{"status":"ok","timestamp":1588411313955,"user_tz":-480,"elapsed":22293,"user":{"displayName":"張仁寬@go.thu","photoUrl":"","userId":"00629718865293591950"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["# Read file from Google Drive\n","\n","from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EW61ujDvA-vH","colab_type":"text"},"source":["# Build Model"]},{"cell_type":"code","metadata":{"id":"cp6ZE8L1A21M","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms    # torchvision contains common utilities for computer vision"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfSuUgJZAzrt","colab_type":"code","colab":{}},"source":["# Class : model definition \n","\n","class Net(nn.Module):  # Inherit from `nn.Module`, define `__init__` & `forward`\n","    def __init__(self):\n","        # Always call the init function of the parent class `nn.Module`\n","        # so that magics can be set up.\n","        super(Net, self).__init__()\n","\n","        # Define the parameters in your network.\n","        # This is achieved by defining the shapes of the multiple layers in the network.\n","\n","        # Define two 2D convolutional layers (1 x 10, 10 x 20 each)\n","        # with convolution kernel of size (5 x 5).\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","\n","        # Define a dropout layer\n","        self.conv2_drop = nn.Dropout2d()\n","\n","        # Define a fully-connected layer (320 x 10)\n","        self.fc = nn.Linear(320, 10)\n","\n","    def forward(self, x):\n","        # Define the network architecture.\n","        # This is achieved by defining how the network forward propagates your inputs\n","\n","        # Input image size: 28 x 28, input channel: 1, batch size (training): 64 \n","\n","        # Input (64 x 1 x 28 x 28) -> Conv1 (64 x 10 x 24 x 24) -> Max Pooling (64 x 10 x 12 x 12) -> ReLU -> ...\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","\n","        # ... -> Conv2 (64 x 20 x 8 x 8) -> Dropout -> Max Pooling (64 x 20 x 4 x 4) -> ReLU -> ...\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","\n","        # ... -> Flatten (64 x 320) -> ...\n","        x = x.view(-1, 320)\n","\n","        # ... -> FC (64 x 10) -> ...\n","        x = self.fc(x)\n","\n","        # ... -> Log Softmax -> Output\n","        return F.log_softmax(x, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OT--Rv8YBrc8","colab_type":"text"},"source":["# Model Training"]},{"cell_type":"code","metadata":{"id":"T4ol9oi6B2ov","colab_type":"code","colab":{}},"source":["#\n","# Modified from PyTorch examples: \n","# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","#\n","\n","import os\n","import argparse\n","#import torch\n","#import torch.nn as nn\n","#import torch.nn.functional as F\n","#import torch.optim as optim\n","#from torchvision import datasets, transforms    # torchvision contains common utilities for computer vision\n","from torch.autograd import Variable\n","from argparse import ArgumentParser\n","\n","#from .model import Net \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L46cguR-BjdX","colab_type":"code","colab":{}},"source":["# Function Module\n","\n","def load_data(train_batch_size, test_batch_size):\n","    \"\"\"Fetch MNIST dataset\n","\n","    MNIST dataset has built-in utilities set up in the `torchvision` package, so we just use the `torchvision.datasets.MNIST` module (http://pytorch.org/docs/master/torchvision/datasets.html#torchvision.datasets.MNIST) to make our lives easier.\n","    \"\"\"\n","\n","    kwargs = {} \n","\n","    # Fetch training data\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=True, download=True,\n","                       transform=transforms.Compose([\n","                           transforms.ToTensor(),\n","                           transforms.Normalize((0.1307,), (0.3081,))\n","                       ])),\n","        batch_size=train_batch_size, shuffle=True, **kwargs)\n","\n","    # Fetch test data\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                           transforms.ToTensor(),\n","                           transforms.Normalize((0.1307,), (0.3081,))\n","                       ])),\n","        batch_size=test_batch_size, shuffle=True, **kwargs)\n","\n","    return (train_loader, test_loader)\n","\n","def train(model, optimizer, epoch, train_loader, log_interval):\n","    # State that you are training the model\n","    model.train()\n","\n","    # Iterate over batches of data\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Wrap the input and target output in the `Variable` wrapper\n","        data, target = Variable(data), Variable(target)\n","\n","        # Clear the gradients, since PyTorch accumulates them\n","        optimizer.zero_grad()\n","\n","        # Forward propagation\n","        output = model(data)\n","\n","        # Calculate negative log likelihood loss\n","        loss = F.nll_loss(output, target)\n","\n","        # Backward propagation\n","        loss.backward()\n","\n","        # Update the gradients\n","        optimizer.step()\n","\n","        # Output debug message\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data.item()))\n","\n","def test(model, test_loader):\n","    # State that you are testing the model; this prevents layers e.g. Dropout to take effect\n","    model.eval()\n","\n","    # Init loss & correct prediction accumulators\n","    test_loss = 0\n","    correct = 0\n","\n","    # Optimize the validation process with `torch.no_grad()`\n","    with torch.no_grad():\n","        # Iterate over data\n","        for data, target in test_loader: # Under `torch.no_grad()`, no need to wrap data & target in `Variable`\n","            # Retrieve output\n","            output = model(data)\n","\n","            # Calculate & accumulate loss\n","            test_loss += F.nll_loss(output, target, reduction='sum').data.item()\n","\n","            # Get the index of the max log-probability (the predicted output label)\n","            pred = output.data.argmax(1)\n","\n","            # If correct, increment correct prediction accumulator\n","            correct += pred.eq(target.data).sum()\n","\n","    # Print out average test loss\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohgK31pUF6Y_","colab_type":"code","outputId":"12743463-0fe4-4e72-af41-a50fa1feb076","executionInfo":{"status":"ok","timestamp":1588411694378,"user_tz":-480,"elapsed":350792,"user":{"displayName":"張仁寬@go.thu","photoUrl":"","userId":"00629718865293591950"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a47a8570573d4c529cf66b01d1a7d346","3e5520eaec744a6292f79f1cee9287ee","c2b1c4ab4bd14438a06430b411fd3bb0","bf59a40484fd4a83960fb3bb66b6920c","0b6f74eaf25b4cfcb9433feee544e2ca","3cb5253bef47460dbc3a6d7f8d9a0015","55106fdaf17540e1bbc81a8332e78b3f","bdb5502719ea499d902043d3912681ae","0c1c38ce34794fbc8b427411a97efe35","ff9224f6e3a14956aa1b7034ab112fc7","19ff6fef8216471ea2af3a713e040560","e913c42f3a8249b38c31adc5ce3409c9","5fbe35ac463249878373915da61b5410","7cc299f0dd384f959e307c14f739f923","b1ee4abffeae4baa92855629a0fe7302","d2a15eaecb1a450cb2be787e0e272a95","1f5fc981f2b14357a830e9fab0a1d998","d9d031a23e9c467fb8dbf8d4444ba0a5","f6d3b197446f4c26bcfb4e52f2ef9c0e","3a089e5e85da49e8ad4b6ad1535d9b79","c1d3ccb922d14f5184ce10b2705b3853","8304600538e248418ebfbe9d8f92ce3e","52ea3f73d3434eba88ec06e720ef8390","6869f33a735341c8897e395a02b45373","adca0ca99df74a68a6e87e783945d5b2","a48a5644afa14bdd96d5db8e77fdfd15","1a7e3014c2aa424993ae88ad1492f059","0a590c433b584cf48020d22db376c071","1f4420659dfe4bc882ffcd365df1f22a","26c29dbdfc6c4b3c81abc9040b6820a9","12ede7fcc62c43f090271fb071176560","43f2f0b667304bb8b4bd84714a2c752d"]}},"source":["# Instantiate the model\n","model = Net()\n","\n","#args =  Namespace(batch_size=64, epochs=10, log_interval=10, lr=0.01, momentum=0.5, seed=1, test_batch_size=1000)\n","\n","parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","parser.add_argument('--batch-size', type=int, default=64, metavar='N',\\\n","                    help='input batch size for training (default: 64)')\n","parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\\\n","                    help='input batch size for testing (default: 1000)')\n","parser.add_argument('--epochs', type=int, default=10, metavar='N',\\\n","                    help='number of epochs to train (default: 10)')\n","parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\\\n","                    help='learning rate (default: 0.01)')\n","parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\\\n","                    help='SGD momentum (default: 0.5)')\n","parser.add_argument('--seed', type=int, default=1, metavar='S',\\\n","                    help='random seed (default: 1)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\\\n","                    help='how many batches to wait before logging training status')\n","\n","#args = parser.parse_args()   # for command line\n","args = parser.parse_args(['--batch-size=64', '--test-batch-siz=1000', '--epochs=10', '--lr=0.01', '--momentum=0.5', '--seed=1', '--log-interval=10'])\n","\n","\n","# Choose SGD as the optimizer, initialize it with the parameters & settings\n","optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","# Load data\n","train_loader, test_loader = load_data(args.batch_size, args.test_batch_size)\n","\n","# Train & test the model\n","for epoch in range(1, args.epochs + 1):\n","     train(model, optimizer, epoch, train_loader, log_interval=args.log_interval)\n","     test(model, test_loader)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a47a8570573d4c529cf66b01d1a7d346","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c1c38ce34794fbc8b427411a97efe35","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f5fc981f2b14357a830e9fab0a1d998","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"adca0ca99df74a68a6e87e783945d5b2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.416496\n","Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.284373\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.152807\n","Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.015532\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.892992\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.519460\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.371071\n","Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.170158\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.778500\n","Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.889671\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.727126\n","Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.626816\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.682499\n","Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.556994\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.571436\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.410163\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.451886\n","Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.511347\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.440953\n","Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.548946\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.308826\n","Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.444564\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.457025\n","Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.380188\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.387922\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.238987\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.307580\n","Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.534173\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.432972\n","Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.391956\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.214652\n","Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.368363\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.254061\n","Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.363169\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.347542\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.343828\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.306022\n","Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.285960\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.287527\n","Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.287903\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.325070\n","Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.303311\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.361958\n","Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.160633\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.305192\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.186966\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.338504\n","Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.267002\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.374217\n","Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.408126\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.425725\n","Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.388473\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.218641\n","Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.137854\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.376646\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.451460\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.626843\n","Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.356081\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.271112\n","Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.169763\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.169944\n","Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.170869\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.163978\n","Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.148329\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.273527\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.443302\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.243168\n","Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.280361\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.129364\n","Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.607807\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.306899\n","Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.192209\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.186777\n","Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.366907\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.287756\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.222806\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.228477\n","Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.281201\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.254002\n","Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.213166\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.181383\n","Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.141501\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.434838\n","Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.240319\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.332424\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.159708\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.399634\n","Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.100652\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.178893\n","Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.152236\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.161530\n","Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.153408\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.101256\n","Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.280467\n","\n","Test set: Average loss: 0.1190, Accuracy: 9644/10000 (96%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.134158\n","Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.104415\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.123518\n","Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.139565\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.318962\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.194877\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.218012\n","Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.165045\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.224034\n","Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.135397\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.194856\n","Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.322621\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.120409\n","Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.208342\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.067105\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.101428\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.202797\n","Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.112916\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.199497\n","Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.101514\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.235639\n","Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.102893\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.175716\n","Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.174012\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.163216\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.252223\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.217469\n","Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.097077\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.152936\n","Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.094719\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.424081\n","Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.291277\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.113660\n","Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.188295\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.171007\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.265495\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.097073\n","Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.171970\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.242201\n","Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.160474\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.232576\n","Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.147362\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.137443\n","Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.167710\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.104014\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.155384\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.107324\n","Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.072946\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.188364\n","Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.188093\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.142485\n","Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.204145\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.149555\n","Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.171685\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.157660\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.124689\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.155025\n","Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.182489\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.155841\n","Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.137492\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.149303\n","Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.271317\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.308666\n","Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.260579\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.153468\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.142852\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.109717\n","Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.407927\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.220497\n","Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.119884\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.123610\n","Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.112309\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.158330\n","Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.073310\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.119014\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.130821\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.301178\n","Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.089619\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.124646\n","Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.184441\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.218067\n","Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.253027\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.087635\n","Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.163060\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.191304\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.234928\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.185881\n","Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.156721\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.150653\n","Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.104728\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.098054\n","Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.132335\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.193727\n","Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.031009\n","\n","Test set: Average loss: 0.0873, Accuracy: 9723/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.210973\n","Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.063319\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.070038\n","Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.097040\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.114043\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.157683\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.361185\n","Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.113224\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.223826\n","Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.199409\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.204986\n","Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.248686\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.280870\n","Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.053078\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.086567\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.129445\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.127626\n","Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.048706\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.162436\n","Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.063248\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.069918\n","Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.196112\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.217870\n","Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.071294\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.121513\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.220392\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.246628\n","Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.118885\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.131733\n","Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.134924\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.122089\n","Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.190339\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.103807\n","Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.102781\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.213867\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.076904\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.148327\n","Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.110439\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.079666\n","Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.142073\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.232707\n","Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.079010\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.278338\n","Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.348590\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.093713\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.100670\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.214719\n","Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.158824\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.240670\n","Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.233944\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.199629\n","Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.188638\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.201863\n","Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.148753\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.105320\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.275130\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.120337\n","Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.049327\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.143239\n","Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.030413\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.209245\n","Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.133368\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.127439\n","Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.075416\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.075047\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.118804\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.149797\n","Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.162780\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.121981\n","Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.335151\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.258561\n","Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.165150\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.049116\n","Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.133249\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.149232\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.116031\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.159820\n","Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.087004\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.079811\n","Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.224528\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.104446\n","Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.066916\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.182360\n","Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.140913\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.089654\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.034587\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.147732\n","Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.063565\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.064207\n","Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.280976\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.066549\n","Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.070561\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.087666\n","Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.098230\n","\n","Test set: Average loss: 0.0690, Accuracy: 9787/10000 (98%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.075874\n","Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.053264\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.175225\n","Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.149904\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.197602\n","Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.114736\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.060138\n","Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.272226\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.060493\n","Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.054329\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.210040\n","Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.188177\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.166118\n","Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.099011\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.116117\n","Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.291330\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.134903\n","Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.171749\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.139808\n","Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.172370\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.136640\n","Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.137158\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.140916\n","Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.043494\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.213384\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.095521\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.246876\n","Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.186035\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.087042\n","Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.071798\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.121938\n","Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.167279\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.122408\n","Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.290480\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.181192\n","Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.176260\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.131767\n","Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.115779\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.139059\n","Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.055333\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.105878\n","Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.139093\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.181827\n","Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.122574\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.044132\n","Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.215308\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.144246\n","Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.198148\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.054291\n","Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.117214\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.148604\n","Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.058405\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.050245\n","Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.168041\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.109311\n","Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.170137\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.112051\n","Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.173861\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.161427\n","Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.095552\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.138283\n","Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.139008\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.069705\n","Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.145659\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.087087\n","Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.030433\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.045848\n","Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.214821\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.093504\n","Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.298136\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.123299\n","Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.077108\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.082591\n","Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.054353\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.119357\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.125078\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.058524\n","Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.067134\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.061876\n","Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.098372\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.196104\n","Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.154397\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.178687\n","Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.115919\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.083998\n","Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.224631\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.041967\n","Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.072698\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.048043\n","Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.195914\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.035229\n","Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.043853\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.116169\n","Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.137479\n","\n","Test set: Average loss: 0.0611, Accuracy: 9810/10000 (98%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.236286\n","Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.147494\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.065618\n","Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.089258\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.210212\n","Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.086161\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.128518\n","Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.066452\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.179035\n","Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.098505\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.246033\n","Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.086029\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.063517\n","Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.032211\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.177672\n","Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.119204\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.086950\n","Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.049564\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.177997\n","Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.073216\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.337535\n","Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.072594\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.034414\n","Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.172853\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.096022\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.044618\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.044189\n","Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.064618\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.166806\n","Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.190988\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.158757\n","Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.244861\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.193217\n","Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.050108\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.162124\n","Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.216337\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.230528\n","Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.061114\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.030490\n","Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.092169\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.092273\n","Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.159001\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.115305\n","Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.205176\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.210558\n","Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.247161\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.175858\n","Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.081062\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.233900\n","Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.125961\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.080897\n","Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.094312\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.131163\n","Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.105450\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.109777\n","Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.353071\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.193817\n","Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.156669\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.030509\n","Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.093084\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.200319\n","Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.036770\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.133316\n","Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.118227\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.255421\n","Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.137821\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.064894\n","Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.188506\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.099526\n","Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.053576\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.156126\n","Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.045408\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.069397\n","Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.122962\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.105895\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.091081\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.151481\n","Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.195196\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.155191\n","Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.081149\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.079840\n","Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.063087\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.080246\n","Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.094271\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.060058\n","Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.127758\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.119941\n","Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.057835\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.143222\n","Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.390946\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.082962\n","Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.138101\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.091725\n","Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.232741\n","\n","Test set: Average loss: 0.0579, Accuracy: 9816/10000 (98%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.093870\n","Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.095936\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.125970\n","Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.056774\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.101829\n","Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.238379\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.108381\n","Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.055847\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.093396\n","Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.011653\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.153887\n","Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.075057\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.095150\n","Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.213166\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.071145\n","Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.039134\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.098435\n","Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.136583\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.097764\n","Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.164522\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.045228\n","Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.155805\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.220033\n","Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.030073\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.091368\n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.036389\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.082095\n","Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.217091\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.078889\n","Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.067133\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.240848\n","Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.110181\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.162971\n","Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.109156\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.106891\n","Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.028809\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.127543\n","Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.053996\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.061624\n","Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.123899\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.041734\n","Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.084894\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.177746\n","Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.050834\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.130147\n","Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.134607\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.062924\n","Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.076839\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.151329\n","Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.086759\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.048702\n","Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.136551\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.092862\n","Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.256279\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.115318\n","Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.072098\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.021307\n","Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.044228\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.045895\n","Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.045993\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.147377\n","Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.016152\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.082678\n","Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.080645\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.060784\n","Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.164478\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.044003\n","Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.171827\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.130282\n","Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.077507\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.098390\n","Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.070972\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.054540\n","Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.015596\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.100178\n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.069950\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.251081\n","Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.122809\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.174109\n","Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.173881\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.096528\n","Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.091676\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.090341\n","Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.151185\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.079228\n","Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.150467\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.036643\n","Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.042499\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.069358\n","Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.078864\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.135879\n","Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.047778\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.045968\n","Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.040444\n","\n","Test set: Average loss: 0.0500, Accuracy: 9842/10000 (98%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.047425\n","Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.101129\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.112816\n","Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.161757\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.047806\n","Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.145434\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.187965\n","Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.125154\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.077929\n","Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.065139\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.068677\n","Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.110038\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.056668\n","Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.321528\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.172150\n","Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.052175\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.105803\n","Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.187890\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.218501\n","Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.058611\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.084652\n","Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.118259\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.068973\n","Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.127629\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.085142\n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.091764\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.174538\n","Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.094346\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.049036\n","Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.163747\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.120397\n","Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.031941\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.129591\n","Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.174911\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.235507\n","Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.150915\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.038702\n","Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.161068\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.106270\n","Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.151195\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.074065\n","Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.072393\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.055206\n","Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.095049\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.045579\n","Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.172121\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.091666\n","Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.069351\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.076969\n","Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.105930\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.024376\n","Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.159078\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.398766\n","Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.024286\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.063639\n","Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.089500\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.082959\n","Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.269621\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.082497\n","Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.140693\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.025037\n","Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.063958\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.047525\n","Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.104175\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.169480\n","Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.176305\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.074433\n","Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.148722\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.035444\n","Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.019603\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.061595\n","Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.049189\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.046482\n","Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.067255\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.057509\n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.075008\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.035967\n","Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.041548\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.064597\n","Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.107053\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.012953\n","Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.512677\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.205946\n","Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.063764\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.090549\n","Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.025906\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.050953\n","Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.097378\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.125512\n","Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.050341\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.073769\n","Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.099153\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.094741\n","Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.236650\n","\n","Test set: Average loss: 0.0488, Accuracy: 9845/10000 (98%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.168131\n","Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.102229\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.124910\n","Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.075653\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.079665\n","Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.028383\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.041304\n","Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.032760\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.106466\n","Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.117012\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.033581\n","Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.219433\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.115525\n","Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.044445\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.072625\n","Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.123176\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.152641\n","Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.040990\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.075444\n","Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.026915\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.142418\n","Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.118492\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.128038\n","Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.076886\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.045816\n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.041626\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.074258\n","Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.095328\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.050589\n","Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.063401\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.088667\n","Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.110030\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.120491\n","Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.021057\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.138148\n","Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.029846\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.031151\n","Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.024533\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.143152\n","Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.063404\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006551\n","Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.212891\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.036312\n","Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.040408\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.085233\n","Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.210379\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.084278\n","Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.048541\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.041795\n","Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.135629\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.229568\n","Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.192719\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.068046\n","Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.050094\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.055120\n","Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.078914\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.028494\n","Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.043802\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.175932\n","Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.072963\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.163227\n","Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.070334\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.203617\n","Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.120281\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.024178\n","Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.028936\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.119224\n","Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.233081\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.113343\n","Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.076529\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.137836\n","Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.121487\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.021467\n","Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.078315\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.090796\n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.112201\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.155249\n","Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.028265\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.030485\n","Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.023360\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.027271\n","Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.095581\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.007150\n","Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.071995\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.174166\n","Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.157269\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.030271\n","Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.024793\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.066345\n","Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.060493\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.090151\n","Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.054591\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.094620\n","Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.068240\n","\n","Test set: Average loss: 0.0448, Accuracy: 9850/10000 (98%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.032245\n","Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.215518\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.043110\n","Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.107548\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.077797\n","Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.219024\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.028543\n","Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.083761\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.019690\n","Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.022815\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.070995\n","Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.142780\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.031100\n","Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.094020\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.100546\n","Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.054404\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.082936\n","Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.030516\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.167412\n","Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.077210\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.038670\n","Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.064348\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.044448\n","Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.110180\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.049503\n","Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.129019\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.051578\n","Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.090183\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.079301\n","Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.050714\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.048958\n","Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.043179\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.062119\n","Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.144193\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.011572\n","Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.081387\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.183330\n","Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.085089\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.093286\n","Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.141844\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.040479\n","Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.043882\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.079322\n","Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.119301\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.133613\n","Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.090842\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.115449\n","Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.039029\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.126234\n","Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.129282\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.129334\n","Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.055024\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.151253\n","Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.167542\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.136523\n","Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.087704\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.164249\n","Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.094763\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.132805\n","Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.054011\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.012602\n","Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.031057\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.016924\n","Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.072679\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.102353\n","Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.022268\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.036813\n","Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.076852\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.133324\n","Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.250457\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.098661\n","Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.046514\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.019010\n","Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.178036\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.023599\n","Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.054705\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.177664\n","Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.067985\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.065964\n","Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.024752\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.038550\n","Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.092866\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.128588\n","Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.046709\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.149024\n","Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.024379\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.131279\n","Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.056364\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.094507\n","Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.096642\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.056914\n","Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.050291\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.138398\n","Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.097604\n","\n","Test set: Average loss: 0.0468, Accuracy: 9854/10000 (99%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.021986\n","Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.062417\n","Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.065763\n","Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.031336\n","Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.067750\n","Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.021962\n","Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.070683\n","Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.023114\n","Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.067348\n","Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.117945\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.071883\n","Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.171489\n","Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.044999\n","Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.022405\n","Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.122455\n","Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.187568\n","Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.015643\n","Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.164068\n","Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.074438\n","Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.100810\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.068680\n","Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.137593\n","Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.025309\n","Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.031638\n","Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.010812\n","Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.076228\n","Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.088095\n","Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.054162\n","Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.030907\n","Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.203166\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.073933\n","Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.059592\n","Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.061185\n","Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.116774\n","Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.018022\n","Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.014375\n","Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.041883\n","Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.075182\n","Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.021463\n","Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.133368\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.139659\n","Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.091700\n","Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.112580\n","Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.062338\n","Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.012278\n","Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.050400\n","Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.032714\n","Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.193214\n","Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.085522\n","Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.123132\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.055079\n","Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.159005\n","Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.088170\n","Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.035652\n","Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.105443\n","Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.040205\n","Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.056424\n","Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.019472\n","Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.122263\n","Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.143981\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.014110\n","Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.045492\n","Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.023077\n","Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.063992\n","Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.093196\n","Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.074119\n","Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.070324\n","Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.042804\n","Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.244607\n","Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.273142\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.045083\n","Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.183981\n","Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.042182\n","Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.060867\n","Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.103058\n","Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.025594\n","Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.018865\n","Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.049305\n","Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.168099\n","Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.061648\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.104328\n","Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.118714\n","Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.085792\n","Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.093745\n","Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.104620\n","Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.045581\n","Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.029260\n","Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.016595\n","Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.081140\n","Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.094713\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.052604\n","Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.019245\n","Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.114565\n","Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.083342\n","\n","Test set: Average loss: 0.0399, Accuracy: 9869/10000 (99%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aofOEUcFOOM_","colab_type":"code","colab":{}},"source":["# Save the model for future use\n","# train & test model - on python command line\n","# $ python -m pytorch.train\n","\n","__file__ = '/content/gdrive/My Drive/Colab Notebooks/MNIST/Pytorch/pytorch/train.py'\n","package_dir = os.path.dirname(os.path.abspath(__file__))\n","model_path = os.path.join(package_dir,'model')\n","torch.save(model.state_dict(), model_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfKnfUv2SEoJ","colab_type":"text"},"source":["# Predicitor"]},{"cell_type":"code","metadata":{"id":"__ecIhuKSHkW","colab_type":"code","outputId":"80028c12-5550-41e9-8014-2919bad6a289","executionInfo":{"status":"ok","timestamp":1588412124504,"user_tz":-480,"elapsed":711,"user":{"displayName":"張仁寬@go.thu","photoUrl":"","userId":"00629718865293591950"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#\n","# Handwritten number predictor\n","#\n","\n","import os\n","import argparse\n","from PIL import Image\n","import torch\n","from torchvision import transforms\n","\n","#from .model import Net\n","\n","\"\"\"Settings\"\"\"\n","\n","# test model interactively on python command line\n","# $ python -m pytorch.app --image=<path-to-image>\n","\n","__file__ = '/content/gdrive/My Drive/Colab Notebooks/MNIST/Pytorch/pytorch/app.py'\n","\n","package_dir = os.path.dirname(os.path.abspath(__file__))\n","default_img_path = os.path.join(package_dir,'test_2.png')\n","\n","parser = argparse.ArgumentParser(description='PyTorch MNIST Predictor')\n","parser.add_argument('--image', type=str, default=default_img_path, metavar='IMG',\n","                            help='image for prediction (default: {})'.format(default_img_path))\n","\n","#args = parser.parse_args() # for python command line using\n","args = parser.parse_args(['--image=' + os.path.join(package_dir,'test_4.png')])\n","\n","\n","\"\"\"Make Prediction\"\"\"\n","\n","# Load model\n","model_path = os.path.join(package_dir,'model')\n","model = Net()\n","model.load_state_dict(torch.load(model_path))\n","\n","# Load & transform image\n","ori_img = Image.open(args.image).convert('L')\n","t = transforms.Compose([\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","img = torch.autograd.Variable(t(ori_img).unsqueeze(0))\n","ori_img.close()\n","\n","# Predict\n","model.eval()\n","output = model(img)\n","pred = output.data.max(1, keepdim=True)[1][0][0]\n","print('Prediction: {}'.format(pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/MNIST/Pytorch/pytorch/test_2.png\n","Prediction: 4\n"],"name":"stdout"}]}]}